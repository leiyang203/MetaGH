# # # #
# import pandas as pd
# import json
#
#
# def process_kg_file(input_file, output_file, x_type_filter, y_type_filter, filter_on):
#     """
#     å¤„ç† kg.csv æ–‡ä»¶ï¼Œç­›é€‰ç¬¦åˆæ¡ä»¶çš„ä¸‰å…ƒç»„ï¼Œå¹¶æŒ‰ç…§å¤´å®ä½“è¿›è¡Œåˆ†ç»„ï¼Œç¡®ä¿æ‰€æœ‰é”®å’Œå€¼å‡ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚
#
#     :param input_file: è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„
#     :param output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
#     :param x_type_filter: å¤´å®ä½“ç±»å‹è¿‡æ»¤æ¡ä»¶
#     :param y_type_filter: å°¾å®ä½“ç±»å‹è¿‡æ»¤æ¡ä»¶
#     :param filter_on: æŒ‰å“ªä¸ªç´¢å¼•è¿›è¡Œç­›é€‰ ('x_index' æˆ– 'y_index')
#     """
#     # è¯»å– kg.csv æ–‡ä»¶
#     df = pd.read_csv(input_file)
#     df = df[df["display_relation"] != "off-label use"]
#
#     # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®
#     filtered_df = df[(df["x_type"] == x_type_filter) & (df["y_type"] == y_type_filter)]
#
#     # ç»Ÿè®¡ filter_on æŒ‡å®šçš„ç´¢å¼•å‡ºç°çš„æ¬¡æ•°
#     index_counts = filtered_df[filter_on].value_counts()
#
#     # ç­›é€‰å‡ºå‡ºç°æ¬¡æ•°åœ¨ 5-20 ä¹‹é—´çš„ç´¢å¼•
#     valid_index = index_counts[(index_counts >= 45) & (index_counts <= 50)].index
#
#     # åªä¿ç•™ç¬¦åˆæ¡ä»¶çš„ç´¢å¼•
#     final_df = filtered_df[filtered_df[filter_on].isin(valid_index)]
#
#     # ç»„ç»‡æ•°æ®ï¼ŒæŒ‰ç…§å¤´å®ä½“åˆ†ç»„ï¼ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
#     entity_dict = {}
#     for row in final_df.itertuples(index=False):
#         x_index = str(row.x_index)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
#         display_relation = str(row.display_relation)
#         y_index = str(row.y_index)
#
#         if x_index not in entity_dict:
#             entity_dict[x_index] = []
#
#         # ç¡®ä¿ä¸‰å…ƒç»„çš„æ¯ä¸ªéƒ¨åˆ†éƒ½æ˜¯å­—ç¬¦ä¸²
#         entity_dict[x_index].append([x_index, display_relation, y_index])
#
#     # ç»Ÿè®¡åŒ…å«å¤šä¸ªä¸‰å…ƒç»„çš„å¤´å®ä½“
#     multiple_triples = {key: val for key, val in entity_dict.items() if len(val) > 1}
#
#     # æ‰“å°åŒ…å«å¤šä¸ªä¸‰å…ƒç»„çš„å¤´å®ä½“
#     print(f"ã€{output_file}ã€‘ä¸­åŒ…å«å¤šä¸ªä¸‰å…ƒç»„çš„å¤´å®ä½“:")
#     for entity, triples in multiple_triples.items():
#         print(f"  {entity}: {len(triples)} ä¸ªä¸‰å…ƒç»„")
#
#     # ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼ˆç¡®ä¿æ‰€æœ‰éƒ¨åˆ†éƒ½ä¸ºå­—ç¬¦ä¸²ï¼‰
#     with open(output_file, "w", encoding="utf-8") as f:
#         json.dump(entity_dict, f, ensure_ascii=False, indent=4)
#
#     print(f"âœ… å·²æˆåŠŸä¿å­˜ {len(entity_dict)} ä¸ªå¤´å®ä½“çš„æ•°æ®åˆ° {output_file}\n")
# # # # # # #
# # # # # # # #
# # # # # # # # âœ… å¤„ç† test_task.json (ç–¾ç—… -> è¯ç‰©)
# process_kg_file(
#     "./primkg/kg.csv",
#     "Y:/primkg-assistant(disease)/test_tasks4.json",
#     x_type_filter="disease",
#     y_type_filter="drug",
#     filter_on="x_index"
# )
#
# # âœ… å¤„ç† test_task_inv.json (è¯ç‰© -> ç–¾ç—…)
# process_kg_file(
#     "./primkg/kg.csv",
#     "Y:/primkg-assistant(disease)/test_tasks_inv4.json",
#     x_type_filter="drug",
#     y_type_filter="disease",
#     filter_on="y_index"
# )
# ---------------------åˆå¹¶è¿™ä¸¤ä¸ªæ–‡ä»¶---------

# import json
#
# with open("./primkg-assistant(disease)/test_tasks.json", "r") as f1, open("./primkg-assistant(disease)/test_tasks_inv.json", "r") as f2:
#     tasks1 = json.load(f1)
#     tasks2 = json.load(f2)
#
# # åˆå¹¶ä¸¤ä¸ª dictï¼ˆé»˜è®¤ key æ˜¯ç–¾ç—… idï¼Œå€¼æ˜¯ä¸‰å…ƒç»„åˆ—è¡¨ï¼‰
# merged_tasks = {}
#
# for k, v in tasks1.items():
#     merged_tasks[k] = v
#
# for k, v in tasks2.items():
#     if k in merged_tasks:
#         merged_tasks[k].extend(v)
#     else:
#         merged_tasks[k] = v
#
# # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
# with open("test_tasks_merged.json", "w", encoding="utf-8") as f:
#     json.dump(merged_tasks, f, ensure_ascii=False, indent=4)
#
# print(f"âœ… åˆå¹¶å®Œæˆï¼Œå…±åŒ…å« {len(merged_tasks)} ä¸ªç–¾ç—…ä»»åŠ¡")

# -------------------------------------------traing_tasks.py-åŒ…æ‹¬äº†æ‰€æœ‰ç±»å‹çš„train_tasks.jsonæ–‡ä»¶åˆ’åˆ†
# import pandas as pd
# import json
#
# # **å®šä¹‰æ–‡ä»¶è·¯å¾„**
# kg_file = "./primkg/kg.csv"
# test_task_file = "Y:/primkg-assistant(disease)/test_tasks4.json"
# test_task_inv_file = "Y:/primkg-assistant(disease)/test_tasks_inv4.json"
# train_task_file = "Y:/primkg-assistant(disease)/train_tasks4.json"
#
# # **è¯»å– kg.csv æ•°æ®**
# print("ğŸ“Œ å¼€å§‹è¯»å– kg.csv æ–‡ä»¶...")
# kg_df = pd.read_csv(kg_file)
# kg_triples = set()
#
# for _, row in kg_df.iterrows():
#     x_index = str(row["x_index"])  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
#     relation = str(row["display_relation"])
#     y_index = str(row["y_index"])
#     kg_triples.add((x_index, relation, y_index))
#
# print(f"âœ… ä» kg.csv æå–ä¸‰å…ƒç»„ {len(kg_triples)} ä¸ª")
#
# # **è¯»å– test_tasks.json æ–‡ä»¶**
# test_triples = set()
# test_heads = set()
#
# print("ğŸ“Œ å¼€å§‹è¯»å– test_tasks.json æ–‡ä»¶...")
# try:
#     with open(test_task_file, 'r', encoding='utf-8') as f:
#         data = json.load(f)
#         for head, triples in data.items():
#             test_heads.add(str(head))  # è®°å½• test_tasks.json ä¸­çš„å¤´å®ä½“
#             for triple in triples:
#                 test_triples.add(tuple(map(str, triple)))  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å½¢å¼çš„ä¸‰å…ƒç»„
# except FileNotFoundError:
#     print(f"âš ï¸ æ–‡ä»¶ {test_task_file} æœªæ‰¾åˆ°ï¼Œè·³è¿‡...")
# except json.JSONDecodeError:
#     print(f"âŒ è§£æ {test_task_file} å¤±è´¥ï¼Œè·³è¿‡...")
#
# print(f"âœ… è¯»å–å®Œæˆï¼Œæµ‹è¯•é›†ä¸‰å…ƒç»„ {len(test_triples)} ä¸ªï¼Œæ¶‰åŠå¤´å®ä½“ {len(test_heads)} ä¸ª")
#
# # **è¯»å– test_tasks_inv.json æ–‡ä»¶**
# print("ğŸ“Œ å¼€å§‹è¯»å– test_tasks_inv.json æ–‡ä»¶...")
# try:
#     with open(test_task_inv_file, 'r', encoding='utf-8') as f:
#         data = json.load(f)
#         for head, triples in data.items():
#             for triple in triples:
#                 test_triples.add(tuple(map(str, triple)))  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å½¢å¼çš„ä¸‰å…ƒç»„
# except FileNotFoundError:
#     print(f"âš ï¸ æ–‡ä»¶ {test_task_inv_file} æœªæ‰¾åˆ°ï¼Œè·³è¿‡...")
# except json.JSONDecodeError:
#     print(f"âŒ è§£æ {test_task_inv_file} å¤±è´¥ï¼Œè·³è¿‡...")
#
# print(f"âœ… è¯»å–å®Œæˆï¼Œæµ‹è¯•é›† (å« inv) ä¸‰å…ƒç»„æ€»æ•° {len(test_triples)} ä¸ª")
#
# # **åˆ é™¤åœ¨ test_task.json å’Œ test_tasks_inv.json ä¸­å­˜åœ¨çš„ä¸‰å…ƒç»„**
# filtered_triples = [t for t in kg_triples if t not in test_triples]
#
# # **ç¡®ä¿ train_tasks.json ä¸­çš„å¤´å®ä½“å’Œå°¾å®ä½“ä¸å±äº test_tasks.json æ–‡ä»¶ä¸­çš„å¤´å®ä½“**
# filtered_triples = [
#     t for t in filtered_triples
#     if t[0] not in test_heads and t[2] not in test_heads
# ]
#
# print(f"âœ… è¿‡æ»¤åå‰©ä½™ä¸‰å…ƒç»„ {len(filtered_triples)} ä¸ª")
#
# # **æ„å»ºå­—å…¸æ ¼å¼**
# train_tasks = {}
# for e1, rel, e2 in filtered_triples:
#     train_tasks.setdefault(e1, []).append([e1, rel, e2])
#
# # **å†™å…¥ train_tasks.json**
# print(f"ğŸ“Œ å¼€å§‹å†™å…¥ {train_task_file} ...")
# with open(train_task_file, 'w', encoding='utf-8') as f:
#     json.dump(train_tasks, f, indent=2, ensure_ascii=False)
#
# print(f"ğŸ¯ {train_task_file} ç”Ÿæˆå®Œæ¯•ï¼Œå…± {len(filtered_triples)} ä¸ªä¸‰å…ƒç»„ï¼Œå¤´å®ä½“ {len(train_tasks)} ä¸ªï¼")




# ------------------------------- traing_tasks.py-åŒ…æ‹¬äº†åªåŒ…æ‹¬äº†è¯ç‰©å’Œç–¾ç—…ç±»å‹çš„train_tasks.jsonæ–‡ä»¶åˆ’åˆ†
# import pandas as pd
# import json
#
# # === æ–‡ä»¶è·¯å¾„å®šä¹‰ ===
# kg_file = "./primkg/kg.csv"
# test_task_file = "./primkg-assistant(disease)/test_tasks1.json"
# test_task_inv_file = "./primkg-assistant(disease)/test_tasks_inv1.json"
# train_task_file = "./primkg-assistant(disease)/train_tasks1.json"
# train_task_w_file = "./primkg-assistant(disease)/train_tasks_w.json"
#
# # === è¯»å– kg.csv æ•°æ®ï¼ˆä¿ç•™å®ä½“ç±»å‹ï¼‰===
# print("ğŸ“Œ å¼€å§‹è¯»å– kg.csv æ–‡ä»¶...")
# kg_df = pd.read_csv(kg_file)
#
# # å°†ä¸‰å…ƒç»„è½¬æ¢ä¸º (x_index, display_relation, y_index) å¹¶ä¿ç•™ç±»å‹ä¿¡æ¯
# kg_triples = set()
# kg_types = {}
#
# for _, row in kg_df.iterrows():
#     x = str(row["x_index"])
#     rel = str(row["display_relation"])
#     y = str(row["y_index"])
#     x_type = str(row["x_type"])
#     y_type = str(row["y_type"])
#
#     kg_triples.add((x, rel, y))
#     kg_types[(x, rel, y)] = (x_type, y_type)
#
# print(f"âœ… ä» kg.csv æå–ä¸‰å…ƒç»„ {len(kg_triples)} ä¸ª")
#
# # === è¯»å– test_tasks.json æ–‡ä»¶ä¸­çš„ä¸‰å…ƒç»„å’Œå¤´å®ä½“ ===
# test_triples = set()
# test_heads = set()
#
# print("ğŸ“Œ å¼€å§‹è¯»å– test_tasks.json æ–‡ä»¶...")
# try:
#     with open(test_task_file, 'r', encoding='utf-8') as f:
#         data = json.load(f)
#         for head, triples in data.items():
#             test_heads.add(str(head))
#             for triple in triples:
#                 test_triples.add(tuple(map(str, triple)))
# except Exception as e:
#     print(f"âš ï¸ è¯»å– {test_task_file} å¤±è´¥ï¼š{e}")
#
# # === è¯»å– test_tasks_inv.json æ–‡ä»¶ ===
# print("ğŸ“Œ å¼€å§‹è¯»å– test_tasks_inv.json æ–‡ä»¶...")
# try:
#     with open(test_task_inv_file, 'r', encoding='utf-8') as f:
#         data = json.load(f)
#         for head, triples in data.items():
#             for triple in triples:
#                 test_triples.add(tuple(map(str, triple)))
# except Exception as e:
#     print(f"âš ï¸ è¯»å– {test_task_inv_file} å¤±è´¥ï¼š{e}")
#
# print(f"âœ… æµ‹è¯•é›†ä¸­æ€»å…±æœ‰ {len(test_triples)} ä¸ªä¸‰å…ƒç»„ï¼Œæ¶‰åŠå¤´å®ä½“ {len(test_heads)} ä¸ª")
#
# # === åˆ é™¤åœ¨ test_task.json ä¸­å·²å­˜åœ¨çš„ä¸‰å…ƒç»„ ===
# filtered_triples = [t for t in kg_triples if t not in test_triples]
#
# # === åˆ é™¤å¤´å®ä½“æˆ–å°¾å®ä½“åœ¨æµ‹è¯•é›†å¤´å®ä½“ä¸­çš„ä¸‰å…ƒç»„ ===
# filtered_triples = [t for t in filtered_triples if t[0] not in test_heads and t[2] not in test_heads]
#
# print(f"âœ… è¿‡æ»¤åå‰©ä½™ä¸‰å…ƒç»„ {len(filtered_triples)} ä¸ª")
#
# # === æ ¹æ®ç±»å‹åˆ’åˆ†ï¼šç–¾ç—…/è¯ç‰©ç›¸å…³çš„æ”¾å…¥ train_tasksï¼Œå…¶ä½™æ”¾å…¥ train_tasks_w ===
# train_tasks = {}
# train_tasks_w = {}
#
# for triple in filtered_triples:
#     x, rel, y = triple
#     x_type, y_type = kg_types[triple]
#
#     is_related = (x_type in {"disease", "drug"} or y_type in {"disease", "drug"})
#
#     if is_related:
#         train_tasks.setdefault(x, []).append([x, rel, y])
#     else:
#         train_tasks_w.setdefault(x, []).append([x, rel, y])
#
# # === å†™å…¥ JSON æ–‡ä»¶ ===
# with open(train_task_file, 'w', encoding='utf-8') as f:
#     json.dump(train_tasks, f, indent=2, ensure_ascii=False)
# print(f"âœ… {train_task_file} å†™å…¥å®Œæˆï¼Œå¤´å®ä½“æ•°ï¼š{len(train_tasks)}")
#
# with open(train_task_w_file, 'w', encoding='utf-8') as f:
#     json.dump(train_tasks_w, f, indent=2, ensure_ascii=False)
# print(f"âœ… {train_task_w_file} å†™å…¥å®Œæˆï¼Œå¤´å®ä½“æ•°ï¼š{len(train_tasks_w)}")
#
# print(
#     f"ğŸ¯ æ€»å…±åˆ’åˆ†ä¸‰å…ƒç»„ï¼šç›¸å…³ï¼ˆ{sum(len(v) for v in train_tasks.values())}ï¼‰ï¼Œæ— å…³ï¼ˆ{sum(len(v) for v in train_tasks_w.values())}ï¼‰")



# # #
# import json
# import random
#
# # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
# input_file = "Y:/primkg-assistant(disease)/test_tasks4.json"
# test_output_file = "Y:/primkg-assistant(disease)/test_tasks4.json"
# dev_output_file = "Y:/primkg-assistant(disease)/dev_tasks4.json"
#
# # è¯»å– test_tasks.json æ–‡ä»¶
# print("ğŸ“Œ å¼€å§‹è¯»å– test_tasks.json...")
# with open(input_file, 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # å°†å¤´å®ä½“æ‰“ä¹±ï¼Œç¡®ä¿éšæœºæ€§
# all_heads = list(data.keys())
# random.shuffle(all_heads)
#
# # è®¡ç®—åˆ’åˆ†æ¯”ä¾‹ï¼ˆ6:4ï¼‰
# test_size = int(len(all_heads) * 0.7)
# test_heads = all_heads[:test_size]  # é€‰å–å‰ 60% çš„å¤´å®ä½“
# dev_heads = all_heads[test_size:]   # å‰©ä½™ 40% çš„å¤´å®ä½“
#
# # **æ„å»ºæ–°æ•°æ®é›†**
# test_data = {head: data[head] for head in test_heads}
# dev_data = {head: data[head] for head in dev_heads}
#
# # **ä¿å­˜ test_task.json æ–‡ä»¶**
# print(f"âœ… ä¿å­˜ {test_output_file}ï¼Œå…±åŒ…å« {len(test_data)} ä¸ªå¤´å®ä½“")
# with open(test_output_file, 'w', encoding='utf-8') as f:
#     json.dump(test_data, f, indent=2, ensure_ascii=False)
#
# # **ä¿å­˜ dev_tasks.json æ–‡ä»¶**
# print(f"âœ… ä¿å­˜ {dev_output_file}ï¼Œå…±åŒ…å« {len(dev_data)} ä¸ªå¤´å®ä½“")
# with open(dev_output_file, 'w', encoding='utf-8') as f:
#     json.dump(dev_data, f, indent=2, ensure_ascii=False)
#
# print("ğŸ¯ æ•°æ®åˆ’åˆ†å®Œæˆï¼")

import json

# def load_json(file):
#     """åŠ è½½ JSON æ–‡ä»¶"""
#     try:
#         with open(file, "r", encoding="utf-8") as f:
#             return json.load(f)
#     except FileNotFoundError:
#         print(f"æ–‡ä»¶ {file} æœªæ‰¾åˆ°ï¼Œè·³è¿‡åŠ è½½ã€‚")
#         return {}
#
# def save_json(data, file):
#     """ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶"""
#     with open(file, "w", encoding="utf-8") as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)
#
# def generate_e1rel_e2(files, output_file):
#     """ç”Ÿæˆ e1rel_e2.json æ–‡ä»¶"""
#     e1rel_e2 = {}
#
#     for file in files:
#         data = load_json(file)
#
#         for e1, triples in data.items():
#             for e1, rel, e2 in triples:
#                 key = f"{e1}{rel}"  # ç»„åˆé”® e1 + rel
#                 if key not in e1rel_e2:
#                     e1rel_e2[key] = set()  # ä½¿ç”¨é›†åˆå»é‡
#                 e1rel_e2[key].add(e2)
#
#     # è½¬æ¢é›†åˆä¸ºåˆ—è¡¨ï¼Œç¡®ä¿ JSON å¯åºåˆ—åŒ–
#     e1rel_e2 = {key: list(values) for key, values in e1rel_e2.items()}
#
#     # ä¿å­˜ä¸º JSON
#     save_json(e1rel_e2, output_file)
#     print(f"æ–‡ä»¶ {output_file} å·²ç”Ÿæˆï¼Œå…± {len(e1rel_e2)} ä¸ªé”®ã€‚")
#
# # éœ€è¦å¤„ç†çš„ JSON æ–‡ä»¶
# # input_files = ["Y:/primkg-assistant(disease)/test_tasks4.json", "Y:/primkg-assistant(disease)/train_tasks4.json", "Y:/primkg-assistant(disease)/dev_tasks4.json"]
# input_files = ['Y:/primkg-assistant(disease)/train_tasks-rare-pro.json','Y:/primkg-assistant(disease)/alcohol withdrawal delirium_indication_test_tasks.json','Y:/primkg-assistant(disease)/delirium_contraindication_test_tasks.json','Y:/primkg-assistant(disease)/alcohol withdrawal delirium_contraindication_test_tasks.json']
# output_file = "Y:/primkg-assistant(disease)/e1rel_e2-rare-pro.json"
# #
# generate_e1rel_e2(input_files, output_file)

# import json
# import json
# from collections import defaultdict
#
# def generate_combined_train_tasks(current_test_file, other_test_files, rare_train_file, output_file):
#     # åŠ è½½ rare train tasks
#     with open(rare_train_file, 'r', encoding='utf-8') as f:
#         rare_tasks = json.load(f)
#
#     # åŠ è½½å½“å‰ test çš„å¤´å®ä½“åˆ—è¡¨
#     with open(current_test_file, 'r', encoding='utf-8') as f:
#         current_test_heads = set(json.load(f).keys())
#
#     # åŠ è½½å…¶ä»– test_tasks çš„å†…å®¹
#     merged_test_tasks = defaultdict(list)
#     for test_file in other_test_files:
#         with open(test_file, 'r', encoding='utf-8') as f:
#             task = json.load(f)
#             for head, triples in task.items():
#                 merged_test_tasks[head].extend(triples)
#
#     # åˆå¹¶ rare_tasksï¼ˆæ’é™¤å½“å‰ test çš„ headï¼‰
#     combined_tasks = defaultdict(list)
#     for head, triples in rare_tasks.items():
#         if head not in current_test_heads:
#             combined_tasks[head].extend(triples)
#
#     # åˆå¹¶å…¶ä»– test_tasks å†…å®¹
#     for head, triples in merged_test_tasks.items():
#         combined_tasks[head].extend(triples)
#
#     # å†™å…¥ç»“æœ
#     with open(output_file, 'w', encoding='utf-8') as f:
#         json.dump(combined_tasks, f, indent=2)
#
#     print(f"âœ” Saved: {output_file}")
#
#
# # ============ æ‰§è¡Œç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶ ============
#
# rare_file = "Y:/primkg-assistant(disease)/train_tasks-rare-pro.json"
# t1 = "Y:/primkg-assistant(disease)/alcohol withdrawal delirium_indication_test_tasks.json"
# t2 = "Y:/primkg-assistant(disease)/alcohol withdrawal delirium_contraindication_test_tasks.json"
# t3 = "Y:/primkg-assistant(disease)/delirium_contraindication_test_tasks.json"
#
# generate_combined_train_tasks(
#     current_test_file=t1,
#     other_test_files=[t2, t3],
#     rare_train_file=rare_file,
#     output_file="Y:/primkg-assistant(disease)/train_tasks-alcohol withdrawal delirium_indication.json"
# )
#
# generate_combined_train_tasks(
#     current_test_file=t2,
#     other_test_files=[t1, t3],
#     rare_train_file=rare_file,
#     output_file="Y:/primkg-assistant(disease)/train_tasks-alcohol withdrawal delirium_contraindication.json"
# )
#
# generate_combined_train_tasks(
#     current_test_file=t3,
#     other_test_files=[t1, t2],
#     rare_train_file=rare_file,
#     output_file="Y:/primkg-assistant(disease)/train_tasks-delirium_contraindication.json"
# )


# import json
# import random
#
# # å®šä¹‰è¾“å…¥æ–‡ä»¶è·¯å¾„
# input_files = [
#     "./primkg-assistant(disease)/test_tasks.json",
#     # "./primkg-assistant(disease)/test_tasks_inv.json",
#     "./primkg-assistant(disease)/train_tasks.json",
#     "./primkg-assistant(disease)/dev_tasks.json"
# ]
# output_file = './primkg-assistant(disease)/e1candidates.json'
#
# # æ„å»ºå¤´å®ä½“åˆ°å°¾å®ä½“çš„æ˜ å°„
# entity_to_tails = {}
# all_entities = set()
#
# # è¯»å–å¹¶æ„å»ºå®ä½“æ˜ å°„
# print("å¼€å§‹è¯»å– JSON æ–‡ä»¶...")
# for input_file in input_files:
#     try:
#         with open(input_file, 'r', encoding='utf-8') as f:
#             data = json.load(f)
#             for head_entity, triples in data.items():
#                 head_entity = str(head_entity)  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
#                 entity_to_tails.setdefault(head_entity, set())
#
#                 for _, _, tail in triples:
#                     tail = str(tail)
#                     entity_to_tails[head_entity].add(tail)
#                     all_entities.add(tail)
#
#                 all_entities.add(head_entity)
#     except json.JSONDecodeError as e:
#         print(f"è§£æ {input_file} æ—¶å‡ºé”™: {e}")
#
# print("JSON æ–‡ä»¶è¯»å–å®Œæˆï¼Œæ€»å®ä½“æ•°:", len(all_entities))
#
# # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿éšæœºé‡‡æ ·
# all_entities = list(all_entities)
#
# # é€æ­¥æ‰©å……æ¯ä¸ªå¤´å®ä½“çš„å€™é€‰å°¾å®ä½“é›†åˆ
# print("å¼€å§‹æ‰©å……å€™é€‰å°¾å®ä½“...")
# processed = 0
# total = len(entity_to_tails)
#
# final_candidates = {}
#
# for head_entity, tails in entity_to_tails.items():
#     processed += 1
#     if processed % 1000 == 0:
#         print(f"å·²å¤„ç† {processed}/{total} ä¸ªå¤´å®ä½“...")
#
#     # **1. é€‰æ‹© 5 ä¸ªæ­£æ ·æœ¬å°¾å®ä½“**
#     # positive_tails = list(tails)
#     # num_positive = min(5, len(positive_tails))  # å¦‚æœæ­£æ ·æœ¬ä¸è¶³ 5 ä¸ªï¼Œåˆ™å…¨éƒ¨ä¿ç•™
#     # selected_positives = random.sample(positive_tails, num_positive)
#
#     # **2. é€‰æ‹© 15 ä¸ªè´Ÿæ ·æœ¬å°¾å®ä½“**
#     available_tails = [e for e in all_entities if e not in tails]
#     num_negative = min(20, len(available_tails))  # ç¡®ä¿ä¸ä¼šè¶…å‡ºå¯é€‰èŒƒå›´
#     selected_negatives = random.sample(available_tails, num_negative)
#
#     # **3. ç»„åˆæ­£è´Ÿæ ·æœ¬ï¼Œç¡®ä¿æ€»æ•°ä¸º 20**
#     final_candidates[head_entity] = selected_negatives
#
# print("å€™é€‰å°¾å®ä½“æ‰©å……å®Œæˆï¼")
#
# # å†™å…¥æ–‡ä»¶
# print(f"å¼€å§‹å†™å…¥ {output_file} ...")
# with open(output_file, 'w', encoding='utf-8') as f:
#     json.dump(final_candidates, f, indent=2, ensure_ascii=False)
#
# print(f"{output_file} ç”Ÿæˆå®Œæ¯•ï¼")

# import json
#
# # å®šä¹‰è¾“å…¥æ–‡ä»¶
# input_files = [
#     "./primkg-assistant(disease)/test_tasks.json",
#     # "./primkg-assistant(disease)/test_tasks_inv.json",
#     "./primkg-assistant(disease)/train_tasks.json",
#     "./primkg-assistant(disease)/dev_tasks.json"
# ]
#
# # å­˜å‚¨å®ä½“çš„é›†åˆï¼ˆå¤´å®ä½“å’Œå°¾å®ä½“ç»Ÿä¸€å»é‡ï¼‰
# entities = set()
#
# # éå†æ‰€æœ‰è¾“å…¥æ–‡ä»¶ï¼Œæå–æ‰€æœ‰å”¯ä¸€çš„å®ä½“
# print("ğŸ“Œ å¼€å§‹éå†æ•°æ®æ–‡ä»¶...")
# for file in input_files:
#     try:
#         with open(file, "r", encoding="utf-8") as f:
#             data = json.load(f)
#             for head, triples in data.items():
#                 entities.add(str(head))  # å¤´å®ä½“åŠ å…¥é›†åˆ
#                 for triple in triples:
#                     _, _, tail = map(str, triple)  # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
#                     entities.add(tail)  # å°¾å®ä½“åŠ å…¥é›†åˆ
#     except FileNotFoundError:
#         print(f"âš ï¸ æ–‡ä»¶ {file} æœªæ‰¾åˆ°ï¼Œè·³è¿‡...")
#     except json.JSONDecodeError:
#         print(f"âŒ è§£æ {file} å¤±è´¥ï¼Œè·³è¿‡...")
#
# print(f"âœ… å®ä½“æ€»æ•°: {len(entities)}")
#
# # **ç¼–å·**
# entity_map = {entity: idx for idx, entity in enumerate(sorted(entities))}
#
# # **ä¿å­˜ä¸º JSON æ–‡ä»¶**
# output_file = "./primkg-assistant(disease)/ent2ids.json"
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(entity_map, f, indent=2, ensure_ascii=False)
#
# print(f"ğŸ¯ æ–‡ä»¶ {output_file} ç”Ÿæˆå®Œæ¯•ï¼")




# import json
# from collections import defaultdict
#
# # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
# test_file = "./primkg-assistant(disease)/test_tasks.json"
# train_file = "./primkg-assistant(disease)/train_tasks.json"
# output_file = "./primkg-assistant(disease)/matching_triples.json"
#
# # è¯»å– test_task.json æ–‡ä»¶
# print("ğŸ“Œ æ­£åœ¨è¯»å– test_task.json...")
# with open(test_file, 'r', encoding='utf-8') as f:
#     test_data = json.load(f)
#
# # è¯»å– train_tasks.json æ–‡ä»¶
# print("ğŸ“Œ æ­£åœ¨è¯»å– train_tasks.json...")
# with open(train_file, 'r', encoding='utf-8') as f:
#     train_data = json.load(f)
#
# # âœ… æ„å»ºå¿«é€ŸæŸ¥è¯¢é›†åˆ
# train_heads = set(train_data.keys())  # å°† train çš„å¤´å®ä½“è½¬æ¢ä¸ºé›†åˆï¼ˆå¿«é€ŸåŒ¹é…ï¼‰
# train_tails = set()
# train_triples_map = defaultdict(list)
#
# # éå† train_tasks.jsonï¼Œå»ºç«‹æ˜ å°„ï¼ˆåŠ å¿«æŸ¥æ‰¾é€Ÿåº¦ï¼‰
# for head, triples in train_data.items():
#     for triple in triples:
#         e1, rel, e2 = map(str, triple)
#         train_tails.add(e2)  # å»ºç«‹å°¾å®ä½“é›†åˆ
#         train_triples_map[e2].append([e1, rel, e2])  # æŒ‰ç…§å°¾å®ä½“å»ºç«‹ç´¢å¼•
#         train_triples_map[e1].append([e1, rel, e2])  # å¤´å®ä½“ä¹Ÿå»ºç«‹ç´¢å¼•ï¼ˆä¾¿äºæŸ¥æ‰¾ï¼‰
#
# print(f"âœ… è®­ç»ƒé›†å¤´å®ä½“æ•°é‡: {len(train_heads)}")
# print(f"âœ… è®­ç»ƒé›†å°¾å®ä½“æ•°é‡: {len(train_tails)}")
#
# # âœ… åŒ¹é… test_task.json æ–‡ä»¶çš„å°¾å®ä½“
# result = defaultdict(list)
#
# for head, triples in test_data.items():
#     for triple in triples:
#         _, _, test_tail = map(str, triple)
#         if test_tail in train_heads or test_tail in train_tails:
#             # å¦‚æœ test_tail åŒ¹é…åˆ° train_tasks.json çš„å¤´å®ä½“æˆ–å°¾å®ä½“ï¼Œä¿å­˜åŒ¹é…ä¸‰å…ƒç»„
#             if test_tail in train_triples_map:
#                 result[test_tail].extend(train_triples_map[test_tail])
#
# # âœ… å»é‡ï¼ˆé˜²æ­¢åŒä¸€ä¸‰å…ƒç»„å¤šæ¬¡åŒ¹é…ï¼‰
# for key in result:
#     result[key] = [list(x) for x in set(tuple(x) for x in result[key])]
#
# # âœ… ä¿å­˜åŒ¹é…ç»“æœ
# with open(output_file, 'w', encoding='utf-8') as f:
#     json.dump(result, f, indent=2, ensure_ascii=False)
#
# print(f"ğŸ¯ åŒ¹é…ç»“æœå·²ä¿å­˜åˆ° {output_file}ï¼Œå…±åŒ¹é…åˆ° {len(result)} ä¸ªå°¾å®ä½“ï¼")



# æ£€æŸ¥æ˜¯å¦æœ‰å¤´å®ä½“åœ¨è®­ç»ƒé›†ä¸­æˆ–å°¾å®ä½“åœ¨è®­ç»ƒé›†ä¸­
# import json
#
# # å®šä¹‰æ–‡ä»¶è·¯å¾„
# test_file = "./primkg-assistant(disease)/test_tasks.json"
# train_file = "./primkg-assistant(disease)/train_tasks.json"
#
# # è¯»å– test_task.json æ–‡ä»¶
# print("ğŸ“Œ æ­£åœ¨è¯»å– test_task.json...")
# with open(test_file, 'r', encoding='utf-8') as f:
#     test_data = json.load(f)
#
# # è¯»å– train_tasks.json æ–‡ä»¶
# print("ğŸ“Œ æ­£åœ¨è¯»å– train_tasks.json...")
# with open(train_file, 'r', encoding='utf-8') as f:
#     train_data = json.load(f)
#
# # âœ… æ„å»ºå¿«é€ŸæŸ¥è¯¢é›†åˆ
# train_heads = set(train_data.keys())  # è®­ç»ƒé›†çš„å¤´å®ä½“é›†åˆ
# train_tails = set()
#
# # éå† train_tasks.jsonï¼Œå»ºç«‹å°¾å®ä½“é›†åˆ
# for triples in train_data.values():
#     for triple in triples:
#         _, _, tail = map(str, triple)
#         train_tails.add(tail)  # å»ºç«‹å°¾å®ä½“é›†åˆ
#
# print(f"âœ… è®­ç»ƒé›†å¤´å®ä½“æ•°é‡: {len(train_heads)}")
# print(f"âœ… è®­ç»ƒé›†å°¾å®ä½“æ•°é‡: {len(train_tails)}")
#
# # âœ… åŒ¹é… test_task.json ä¸­çš„å¤´å®ä½“
# matched_heads = {}
#
# for head in test_data.keys():
#     if head in train_heads:
#         matched_heads[head] = "åœ¨è®­ç»ƒé›†å¤´å®ä½“ä¸­"
#     elif head in train_tails:
#         matched_heads[head] = "åœ¨è®­ç»ƒé›†å°¾å®ä½“ä¸­"
#
# # âœ… æ‰“å°åŒ¹é…ç»“æœ
# if matched_heads:
#     print("\nğŸ¯ åŒ¹é…åˆ°çš„å¤´å®ä½“å¦‚ä¸‹ï¼š")
#     for head, position in matched_heads.items():
#         print(f"å¤´å®ä½“: {head} â” {position}")
# else:
#     print("\nğŸš« æœªæ‰¾åˆ°åŒ¹é…çš„å¤´å®ä½“")
#
# print(f"\nâœ… åŒ¹é…åˆ° {len(matched_heads)} ä¸ªå¤´å®ä½“ï¼")

import json

# def count_tasks(file_path):
#     with open(file_path, 'r') as f:
#         tasks = json.load(f)
#     num_heads = len(tasks.keys())
#     num_triples = sum(len(triples) for triples in tasks.values())
#     return num_heads, num_triples
#
# train_heads, train_triples = count_tasks('./primkg-assistant(disease)/train_tasks.json')
# dev_heads, dev_triples = count_tasks('./primkg-assistant(disease)/dev_tasks.json')
# # å¦‚æœæœ‰ test_tasks.json æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ç»Ÿè®¡ï¼š
# test_heads, test_triples = count_tasks('./primkg-assistant(disease)/test_tasks.json')
#
# print("train_tasks.json: {} head entities, {} triples".format(train_heads, train_triples))
# print("dev_tasks.json: {} head entities, {} triples".format(dev_heads, dev_triples))
# print("test_tasks.json: {} head entities, {} triples".format(test_heads, test_triples))
#


# import json
#
# def count_tasks_with_relations_and_tails(file_path):
#     with open(file_path, 'r') as f:
#         tasks = json.load(f)
#
#     num_heads = len(tasks)
#     num_triples = 0
#     indication_count = 0
#     contraindication_count = 0
#     tail_entities = set()
#
#     for triples in tasks.values():
#         num_triples += len(triples)
#         for triple in triples:
#             rel = triple[1]
#             tail = triple[2]
#             tail_entities.add(tail)
#             if rel == 'indication':
#                 indication_count += 1
#             elif rel == 'contraindication':
#                 contraindication_count += 1
#
#     num_tail_entities = len(tail_entities)
#     return num_heads, num_triples, indication_count, contraindication_count, num_tail_entities
#
# # ä½¿ç”¨å‡½æ•°
# # train_heads, train_triples, train_ind, train_contra, train_tails = count_tasks_with_relations_and_tails('Y:/primkg-assistant(disease)/train_tasks2.json')
# # dev_heads, dev_triples, dev_ind, dev_contra, dev_tails = count_tasks_with_relations_and_tails('Y:/primkg-assistant(disease)/dev_tasks4.json')
# test_heads, test_triples, test_ind, test_contra, test_tails = count_tasks_with_relations_and_tails('Y:/primkg-assistant(disease)/test_tasks4.json')
# #
# # print(f"train_tasks.json: {train_heads} head entities, {train_triples} triples, {train_tails} unique tail entities")
# # print(f"  - indication: {train_ind}, contraindication: {train_contra}")
#
# # print(f"dev_tasks.json: {dev_heads} head entities, {dev_triples} triples, {dev_tails} unique tail entities")
# # print(f"  - indication: {dev_ind}, contraindication: {dev_contra}")
#
# print(f"test_tasks.json: {test_heads} head entities, {test_triples} triples, {test_tails} unique tail entities")
# print(f"  - indication: {test_ind}, contraindication: {test_contra}")


import pandas as pd
import json

def create_train_test_split(kg_file, train_output, test_output, test_heads, test_relations):
    df = pd.read_csv(kg_file)
    df = df[df["display_relation"] != "off-label use"]

    df["x_index"] = df["x_index"].astype(str)
    df["y_index"] = df["y_index"].astype(str)
    df["display_relation"] = df["display_relation"].astype(str)
    df["x_name"] = df["x_name"].astype(str)

    # âœ… æ·»åŠ å¤šé‡æ¡ä»¶è¿‡æ»¤ï¼šx_type, x_name, display_relation
    test_df = df[
        (df["x_type"] == "disease") &
        (df["x_name"].isin(test_heads)) &
        (df["display_relation"].isin(test_relations))
    ]

    train_df = df.drop(index=test_df.index)

    def build_triple_dict(df):
        triple_dict = {}
        for row in df.itertuples(index=False):
            head = row.x_index
            rel = row.display_relation
            tail = row.y_index
            triple = [head, rel, tail]
            if head not in triple_dict:
                triple_dict[head] = []
            triple_dict[head].append(triple)
        return triple_dict

    test_triples = build_triple_dict(test_df)
    train_triples = build_triple_dict(train_df)

    inverse_set = set()
    for triples in test_triples.values():
        for h, r, t in triples:
            inverse_set.add((t, r, h))

    cleaned_train_triples = {}
    for head, triples in train_triples.items():
        cleaned = [trip for trip in triples if (trip[2], trip[1], trip[0]) not in inverse_set]
        if cleaned:
            cleaned_train_triples[head] = cleaned

    with open(test_output, "w", encoding="utf-8") as f_test:
        json.dump(test_triples, f_test, ensure_ascii=False, indent=4)
    print(f"âœ… æµ‹è¯•é›†ä¿å­˜æˆåŠŸï¼ŒåŒ…å« {len(test_triples)} ä¸ªå¤´å®ä½“ï¼š{test_output}")

    with open(train_output, "w", encoding="utf-8") as f_train:
        json.dump(cleaned_train_triples, f_train, ensure_ascii=False, indent=4)
    print(f"âœ… è®­ç»ƒé›†ä¿å­˜æˆåŠŸï¼ŒåŒ…å« {len(cleaned_train_triples)} ä¸ªå¤´å®ä½“ï¼š{train_output}")

# âœ… è°ƒç”¨ä¸»å‡½æ•°
kg_file = "/home/ubuntu/YL/primkg-assistant(disease)/kg.csv"
train_output = "/home/ubuntu/YL/primkg-assistant(disease)/train_cystic fibrosis_tasks.json"
test_output = "/home/ubuntu/YL/primkg-assistant(disease)/test_cystic fibrosis_tasks.json"

test_heads = ["cystic fibrosis"]
test_relations = ["indication", "contraindication"]

create_train_test_split(kg_file, train_output, test_output, test_heads, test_relations)
